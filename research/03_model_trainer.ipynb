{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 11:41:49.870730: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-29 11:41:50.185147: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-29 11:41:50.189250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-29 11:41:52.090723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/highfrezh/Desktop/AI-ML-ENG/DL PROJECTS/end-to-end-brain-tumor-classification-using-MRI-Dataset/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/highfrezh/Desktop/AI-ML-ENG/DL PROJECTS/end-to-end-brain-tumor-classification-using-MRI-Dataset'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    split_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    testing_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    labels: list\n",
    "    classes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH):\n",
    "        \"\"\"\n",
    "        Handles reading configuration and parameter files.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config[\"artifacts_root\"]])\n",
    "\n",
    "    def get_training_config(self):\n",
    "        \"\"\"\n",
    "        Fetch training configuration and return as a structured object.\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "\n",
    "        training_data_path = os.path.join(self.config.data_ingestion.unzip_dir, \"Training\")\n",
    "        testing_data_path = os.path.join(self.config.data_ingestion.unzip_dir, \"Testing\")\n",
    "\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            split_dir=Path(training.split_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data_path),\n",
    "            testing_data=Path(testing_data_path),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            labels=params.LABELS,\n",
    "            classes=params.CLASSES,\n",
    "        )\n",
    "        \n",
    "        return training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, training_config):\n",
    "        \"\"\"\n",
    "        Handles data loading and preprocessing based on the training configuration.\n",
    "        \"\"\"\n",
    "        self.training_config = training_config\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the training and testing data.\n",
    "        \"\"\"\n",
    "        X, Y = [], []\n",
    "\n",
    "        # Load training data\n",
    "        for label in self.training_config.labels:\n",
    "            folder_path = os.path.join(self.training_config.training_data, label)\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img = cv2.imread(os.path.join(folder_path, img_name))\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                X.append(img)\n",
    "                Y.append(self.training_config.labels.index(label))  # Map label to numeric value\n",
    "\n",
    "        # Load testing data\n",
    "        for label in self.training_config.labels:\n",
    "            folder_path = os.path.join(self.training_config.testing_data, label)\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img = cv2.imread(os.path.join(folder_path, img_name))\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                X.append(img)\n",
    "                Y.append(self.training_config.labels.index(label))  # Map label to numeric value\n",
    "\n",
    "        # Convert to NumPy arrays and preprocess\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)        \n",
    "\n",
    "        X = preprocess_input(X)\n",
    "        # Debug shapes\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        print(f\"Y shape: {Y.shape}\")\n",
    "\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    def prepare_data(self, X, Y, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into training and validation sets and one-hot encode the labels.\n",
    "        \"\"\"\n",
    "        # Split data\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        num_classes = len(self.training_config.labels)\n",
    "        Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "        Y_val = to_categorical(Y_val, num_classes=num_classes)\n",
    "\n",
    "        # Debug shapes\n",
    "        print(f\"X_train shape: {X_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}\")\n",
    "        print(f\"Y_train shape: {Y_train.shape}\")\n",
    "        print(f\"Y_val shape: {Y_val.shape}\")\n",
    "\n",
    "        # Create the save directory if it doesn't exist\n",
    "        os.makedirs(self.training_config.split_dir, exist_ok=True)\n",
    "\n",
    "        # Save the datasets using pickle\n",
    "        with open(os.path.join(self.training_config.split_dir, \"train_data.pkl\"), \"wb\") as f:\n",
    "            pickle.dump((X_train, Y_train), f)\n",
    "        with open(os.path.join(self.training_config.split_dir, \"val_data.pkl\"), \"wb\") as f:\n",
    "            pickle.dump((X_val, Y_val), f)\n",
    "\n",
    "        return X_train, Y_train, X_val, Y_val, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, training_config: TrainingConfig, X_train, Y_train, X_val, Y_val,):\n",
    "        \"\"\"\n",
    "        Initialize the training process with configuration and preprocessed data.\n",
    "        \"\"\"\n",
    "        self.config = training_config\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.model = None  # Placeholder for the loaded model\n",
    "\n",
    "    def load_base_model(self):\n",
    "        \"\"\"\n",
    "        Load the pre-saved base model from the specified path.\n",
    "        \"\"\"\n",
    "        print(f\"Loading base model from: {self.config.updated_base_model_path}\")\n",
    "        self.model = tf.keras.models.load_model(self.config.updated_base_model_path)\n",
    "        print(\"Base model loaded successfully.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        \"\"\"\n",
    "        Save the trained model to the specified path.\n",
    "        \"\"\"\n",
    "        model.save(path)\n",
    "        print(f\"Model saved successfully at: {path}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the loaded model using preprocessed datasets.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call 'load_base_model()' before training.\")\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Starting training...\")\n",
    "        history = self.model.fit(\n",
    "            self.X_train, \n",
    "            self.Y_train,\n",
    "            validation_data=(self.X_val, self.Y_val),\n",
    "            epochs=self.config.params_epochs,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "        )\n",
    "        print(\"Training completed successfully.\")\n",
    "\n",
    "        # Save the trained model\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-29 11:41:57,025: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-12-29 11:41:57,032: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-12-29 11:41:57,034: INFO: common: created directory at: artifacts]\n",
      "[2024-12-29 11:41:57,035: INFO: common: created directory at: artifacts/training]\n",
      "X shape: (7023, 224, 224, 3)\n",
      "Y shape: (7023,)\n",
      "X_train shape: (5618, 224, 224, 3)\n",
      "X_val shape: (1405, 224, 224, 3)\n",
      "Y_train shape: (5618, 4)\n",
      "Y_val shape: (1405, 4)\n",
      "Loading base model from: artifacts/prepare_base_model/base_model_updated.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 11:43:04.001612: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2024-12-29 11:43:04.082865: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2024-12-29 11:43:04.147195: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    preprocess_data = DataPreprocessor(training_config)\n",
    "    X, Y = preprocess_data.load_data()\n",
    "    X_train, Y_train, X_val, Y_val,num_classes = preprocess_data.prepare_data(X, Y)    \n",
    "    training = Training(training_config,X_train, Y_train, X_val, Y_val,)\n",
    "    training.load_base_model()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
